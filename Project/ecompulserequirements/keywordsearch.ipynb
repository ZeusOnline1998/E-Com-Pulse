{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f3c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    keyword_id  search_category  \\\n",
      "0            1                0   \n",
      "1            1                0   \n",
      "2            1                0   \n",
      "3            1                0   \n",
      "4            1                0   \n",
      "5            1                0   \n",
      "6            1                0   \n",
      "7            1                0   \n",
      "8            1                0   \n",
      "9            1                0   \n",
      "10           1                0   \n",
      "11           1                0   \n",
      "12           1                0   \n",
      "13           1                0   \n",
      "14           1                0   \n",
      "15           1                0   \n",
      "16           1                0   \n",
      "17           1                0   \n",
      "18           1                0   \n",
      "19           1                0   \n",
      "20           1                0   \n",
      "21           1                0   \n",
      "22           1                0   \n",
      "\n",
      "                                         product_name  product_rank  \\\n",
      "0   dr. sheth's ceramide & vitamin c sunscreen spf...             1   \n",
      "1   plum green tea daylight sunscreen gel | spf fo...             2   \n",
      "2   the derma co 1% hyaluronic sunscreen aqua ultr...             3   \n",
      "3   la shield pollution protect mineral sunscreen ...             4   \n",
      "4   minimalist sunscreen spf 50 lightweight with m...             5   \n",
      "5       neutrogena sunscreen spf 50+ - 80.0 grams oil             6   \n",
      "6   the derma co 1% hyaluronic sunscreen aqua ultr...             7   \n",
      "7   dot & key watermelon hyaluronic cooling sunscr...             8   \n",
      "8   dot & key vitamin c + e super bright sunscreen...             9   \n",
      "9   aqualogica glow+ dewy sunscreen spf 50 pa+++ f...            10   \n",
      "10  la shield pollution protect mineral sunscreen ...            35   \n",
      "11  dr. sheth's ceramide & vitamin c sunscreen spf...             1   \n",
      "12  plum green tea daylight sunscreen gel | spf fo...             2   \n",
      "13  the derma co 1% hyaluronic sunscreen aqua ultr...             3   \n",
      "14  la shield pollution protect mineral sunscreen ...             4   \n",
      "15  minimalist sunscreen spf 50 lightweight with m...             5   \n",
      "16      neutrogena sunscreen spf 50+ - 80.0 grams oil             6   \n",
      "17  the derma co 1% hyaluronic sunscreen aqua ultr...             7   \n",
      "18  fixderma shadow sunscreen spf 30+ gel sunscree...             8   \n",
      "19  neutrogena ultra sheer sunscreen, spf 50+, ult...             9   \n",
      "20  aqualogica glow+ dewy sunscreen spf 50 pa+++ f...            10   \n",
      "21  la shield pollution protect mineral sunscreen ...            28   \n",
      "22  la shield spf 40 & pa+++ mineral based sunscre...            45   \n",
      "\n",
      "    crawl_date  platform_id  area_id  \n",
      "0   2023-09-06            1   500004  \n",
      "1   2023-09-06            1   500004  \n",
      "2   2023-09-06            1   500004  \n",
      "3   2023-09-06            1   500004  \n",
      "4   2023-09-06            1   500004  \n",
      "5   2023-09-06            1   500004  \n",
      "6   2023-09-06            1   500004  \n",
      "7   2023-09-06            1   500004  \n",
      "8   2023-09-06            1   500004  \n",
      "9   2023-09-06            1   500004  \n",
      "10  2023-09-06            1   500004  \n",
      "11  2023-09-06            1   500004  \n",
      "12  2023-09-06            1   500004  \n",
      "13  2023-09-06            1   500004  \n",
      "14  2023-09-06            1   500004  \n",
      "15  2023-09-06            1   500004  \n",
      "16  2023-09-06            1   500004  \n",
      "17  2023-09-06            1   500004  \n",
      "18  2023-09-06            1   500004  \n",
      "19  2023-09-06            1   500004  \n",
      "20  2023-09-06            1   500004  \n",
      "21  2023-09-06            1   500004  \n",
      "22  2023-09-06            1   500004  \n",
      "    keyword_id  search_category  \\\n",
      "0            1                0   \n",
      "1            1                0   \n",
      "2            1                0   \n",
      "3            1                0   \n",
      "4            1                0   \n",
      "5            1                0   \n",
      "6            1                0   \n",
      "7            1                0   \n",
      "8            1                0   \n",
      "9            1                0   \n",
      "10           1                0   \n",
      "11           1                0   \n",
      "12           1                0   \n",
      "13           1                0   \n",
      "14           1                0   \n",
      "15           1                0   \n",
      "16           1                0   \n",
      "17           1                0   \n",
      "18           1                0   \n",
      "19           1                0   \n",
      "20           1                0   \n",
      "21           1                0   \n",
      "22           1                0   \n",
      "\n",
      "                                         product_name  product_rank  \\\n",
      "0   dr. sheth's ceramide & vitamin c sunscreen spf...             1   \n",
      "1   plum green tea daylight sunscreen gel | spf fo...             2   \n",
      "2   the derma co 1% hyaluronic sunscreen aqua ultr...             3   \n",
      "3   the derma co 1% hyaluronic tinted sunscreen ge...             4   \n",
      "4   minimalist sunscreen spf 50 lightweight with m...             5   \n",
      "5       neutrogena sunscreen spf 50+ - 80.0 grams oil             6   \n",
      "6   the derma co 1% hyaluronic sunscreen aqua ultr...             7   \n",
      "7   dot & key watermelon hyaluronic cooling sunscr...             8   \n",
      "8   dot & key vitamin c + e super bright sunscreen...             9   \n",
      "9   aqualogica radiance+ dewy sunscreen cream with...            10   \n",
      "10  la shield pollution protect mineral sunscreen ...            35   \n",
      "11  dr. sheth's ceramide & vitamin c sunscreen spf...             1   \n",
      "12  plum green tea daylight sunscreen gel | spf fo...             2   \n",
      "13  the derma co 1% hyaluronic sunscreen aqua ultr...             3   \n",
      "14  the derma co 1% hyaluronic tinted sunscreen ge...             4   \n",
      "15  minimalist sunscreen spf 50 lightweight with m...             5   \n",
      "16      neutrogena sunscreen spf 50+ - 80.0 grams oil             6   \n",
      "17  the derma co 1% hyaluronic sunscreen aqua ultr...             7   \n",
      "18  dot & key vitamin c + e super bright sunscreen...             8   \n",
      "19  aqualogica radiance+ dewy sunscreen cream with...             9   \n",
      "20  dot & key watermelon hyaluronic cooling sunscr...            10   \n",
      "21  la shield pollution protect mineral sunscreen ...            33   \n",
      "22  la shield spf 40 & pa+++ mineral based sunscre...            50   \n",
      "\n",
      "    crawl_date  platform_id  area_id  \n",
      "0   2023-09-06            1   400093  \n",
      "1   2023-09-06            1   400093  \n",
      "2   2023-09-06            1   400093  \n",
      "3   2023-09-06            1   400093  \n",
      "4   2023-09-06            1   400093  \n",
      "5   2023-09-06            1   400093  \n",
      "6   2023-09-06            1   400093  \n",
      "7   2023-09-06            1   400093  \n",
      "8   2023-09-06            1   400093  \n",
      "9   2023-09-06            1   400093  \n",
      "10  2023-09-06            1   400093  \n",
      "11  2023-09-06            1   400093  \n",
      "12  2023-09-06            1   400093  \n",
      "13  2023-09-06            1   400093  \n",
      "14  2023-09-06            1   400093  \n",
      "15  2023-09-06            1   400093  \n",
      "16  2023-09-06            1   400093  \n",
      "17  2023-09-06            1   400093  \n",
      "18  2023-09-06            1   400093  \n",
      "19  2023-09-06            1   400093  \n",
      "20  2023-09-06            1   400093  \n",
      "21  2023-09-06            1   400093  \n",
      "22  2023-09-06            1   400093  \n",
      "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"input.Pke_EE\"}\n",
      "  (Session info: headless chrome=116.0.5845.141); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF70E4B52A2+57122]\n",
      "\t(No symbol) [0x00007FF70E42EA92]\n",
      "\t(No symbol) [0x00007FF70E2FE3AB]\n",
      "\t(No symbol) [0x00007FF70E337D3E]\n",
      "\t(No symbol) [0x00007FF70E337E2C]\n",
      "\t(No symbol) [0x00007FF70E370B67]\n",
      "\t(No symbol) [0x00007FF70E35701F]\n",
      "\t(No symbol) [0x00007FF70E36EB82]\n",
      "\t(No symbol) [0x00007FF70E356DB3]\n",
      "\t(No symbol) [0x00007FF70E32D2B1]\n",
      "\t(No symbol) [0x00007FF70E32E494]\n",
      "\tGetHandleVerifier [0x00007FF70E75EF82+2849794]\n",
      "\tGetHandleVerifier [0x00007FF70E7B1D24+3189156]\n",
      "\tGetHandleVerifier [0x00007FF70E7AACAF+3160367]\n",
      "\tGetHandleVerifier [0x00007FF70E546D06+653702]\n",
      "\t(No symbol) [0x00007FF70E43A208]\n",
      "\t(No symbol) [0x00007FF70E4362C4]\n",
      "\t(No symbol) [0x00007FF70E4363F6]\n",
      "\t(No symbol) [0x00007FF70E4267A3]\n",
      "\tBaseThreadInitThunk [0x00007FFC212026AD+29]\n",
      "\tRtlUserThreadStart [0x00007FFC21FEAA68+40]\n",
      "\n",
      "cannot convert the series to <class 'int'>\n",
      "cannot convert the series to <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "# import pangres\n",
    "\n",
    "proxies = {\n",
    "    \"http\": \"http://125.141.200.53:80\",\n",
    "    \"https\": \"https://125.141.200.53:80\",\n",
    "    \"ftp\": \"ftp://10.10.1.10:3128\"\n",
    "}\n",
    "\n",
    "proxies_list = [\"128.199.109.241:8080\",\"113.53.230.195:3128\",\"125.141.200.53:80\",\"125.141.200.14:80\",\"128.199.200.112:138\",\"149.56.123.99:3128\",\"128.199.200.112:80\",\"125.141.200.39:80\",\"134.213.29.202:4444\"]\n",
    "\n",
    "website_urls = {'Amazon': 'https://www.amazon.in/',\n",
    "               'Flipkart': 'https://www.flipkart.com/',\n",
    "               'Nykaa': 'https://www.nykaa.com/'}\n",
    "\n",
    "engine = sqlalchemy.create_engine('postgresql://postgres:pass@localhost:5432/glenmark')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# Variables for finding components of soup\n",
    "# \n",
    "\n",
    "# for get_amazon_links()\n",
    "search_box_id = \"twotabsearchtextbox\"\n",
    "\n",
    "# for get_flipkart_links()\n",
    "close_button_selector = \"button._2KpZ6l._2doB4z\"\n",
    "search_box_selector1 = \"input.Pke_EE\"\n",
    "search_box_selector2 = \"input._3704LK\"\n",
    "\n",
    "# for get_nykaa_links()\n",
    "close_button_xpath = \"//button[@class='modal-close']\"\n",
    "ny_search_box_selector = \"input.css-1upamjb\"\n",
    "\n",
    "# for amazon_product_search()\n",
    "tag_name_for_product_names = \"span\"\n",
    "class_for_product_names = \"a-size-base-plus a-color-base a-text-normal\"\n",
    "\n",
    "# for flipkart_product_search()\n",
    "fl_tag_name_for_product_names = \"a\"\n",
    "fl_class_for_product_names = \"s1Q9rs\"\n",
    "\n",
    "# for nykaa_product_search()\n",
    "ny_tag_name_for_product_names = \"div\"\n",
    "ny_class_for_product_names = \"css-xrzmfa\"\n",
    "\n",
    "# \n",
    "# End of variables\n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for creating soup object\n",
    "# Takes one url as an input and returns soup of that page\n",
    "def get_soup(url):\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}\n",
    "    req = Request(url, headers=header)\n",
    "    Response = urlopen(req, timeout=time.sleep(0.5 * random.random()))\n",
    "    soup = BeautifulSoup(Response, 'html.parser')\n",
    "    time.sleep(1.5 * random.random())\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for getting links\n",
    "# Takes given keywords as an input, visits amazon, enters the keywords and fetches the links for creating soup object\n",
    "def get_amazon_links(keywords, pincode):\n",
    "    keyword_urls = {}\n",
    "    for keyword in keywords:\n",
    "#         driver = webdriver.Chrome('chromedriver_win32')\n",
    "        chrome_options = Options()\n",
    "        # chrome_options.add_argument(\"--headless\")\n",
    "        while True:\n",
    "            try:\n",
    "                driver = webdriver.Chrome(options=chrome_options)\n",
    "                driver.get(website_urls['Amazon'])\n",
    "                search_box = driver.find_element(By.ID, search_box_id)\n",
    "                char_array = list(keyword)\n",
    "                for char in char_array:\n",
    "                    search_box.send_keys(char)\n",
    "                    time.sleep(0.5 * random.random())\n",
    "        #         search_box.send_keys(keyword)\n",
    "                search_box.send_keys(Keys.RETURN)\n",
    "                product_url = driver.current_url\n",
    "                keyword_urls[keyword] = product_url\n",
    "                driver.quit()\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    return keyword_urls\n",
    "# get_amazon_links(keywords)\n",
    "# print(keyword_urls)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for getting links\n",
    "# Takes given keywords as an input, visits flipkart, enters the keywords and fetches the links for creating soup object\n",
    "def get_flipkart_links(keywords, pincode):\n",
    "    keyword_urls = {}\n",
    "    for keyword in keywords:\n",
    "#         driver = webdriver.Chrome('chromedriver_win32')\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(website_urls['Flipkart'])\n",
    "        try:\n",
    "            close_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, close_button_selector))\n",
    "            )\n",
    "            close_button.click()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            search_box = driver.find_element(By.CSS_SELECTOR, search_box_selector1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            search_box = driver.find_element(By.CSS_SELECTOR, search_box_selector2)\n",
    "        char_array = list(keyword)\n",
    "        for char in char_array:\n",
    "            search_box.send_keys(char)\n",
    "            time.sleep(0.5 * random.random())\n",
    "#         search_box.send_keys(keyword)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        product_url = driver.current_url\n",
    "        keyword_urls[keyword] = product_url\n",
    "        driver.quit()\n",
    "    return keyword_urls\n",
    "# get_flipkart_links(keywords)\n",
    "# print(keyword_urls_fl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for getting links\n",
    "# Takes given keywords as an input, visits nykaa, enters the keywords and fetches the links for creating soup object\n",
    "def get_nykaa_links(keywords, pincode):\n",
    "    keyword_urls = {}\n",
    "    for keyword in keywords:\n",
    "#         driver = webdriver.Chrome('chromedriver_win32')\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(website_urls['Nykaa'])\n",
    "        try:\n",
    "            close_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, close_button_xpath))\n",
    "            )\n",
    "            close_button.click()\n",
    "        except:\n",
    "            pass\n",
    "        search_box = driver.find_element(By.CSS_SELECTOR, ny_search_box_selector)\n",
    "        char_array = list(keyword)\n",
    "        for char in char_array:\n",
    "            search_box.send_keys(char)\n",
    "            time.sleep(0.5 * random.random())\n",
    "#         search_box.send_keys(keyword)\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        product_url = driver.current_url\n",
    "        keyword_urls[keyword] = product_url\n",
    "        driver.quit()\n",
    "    return keyword_urls\n",
    "# get_nykaa_links(keywords)\n",
    "# print(keyword_urls_ny)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method for fetching product details and storing it\n",
    "# Takes 2 inputs\n",
    "# 1. Url whose soup object needs to be created\n",
    "# 2. Keyword dataframe\n",
    "# The method gathers the required data, creates a dataframe and stores the data in the db.\n",
    "def amazon_product_search(urls, amazon_keywords_df, pincode):\n",
    "    Keyword_id = []\n",
    "    search_category = []\n",
    "    Date = []\n",
    "    Keyword = []\n",
    "    Product_name = []\n",
    "    Ranking = []\n",
    "    \n",
    "    keywords = amazon_keywords_df['keyword'].tolist()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        single_keyword_df = amazon_keywords_df[amazon_keywords_df['keyword']==keyword]\n",
    "        # keyword_id = int(single_keyword_df.iloc[index,'id'])\n",
    "        try:\n",
    "            keyword_id = int(single_keyword_df['id'].iloc[0])\n",
    "        except:\n",
    "            pass\n",
    "#         print(keyword_id)\n",
    "#         print(\"after keyword_id >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        \n",
    "        soup = get_soup(urls[keyword])\n",
    "\n",
    "        product_names = soup.find_all(tag_name_for_product_names, class_=class_for_product_names)\n",
    "#         print('printing product names>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#         print(product_names)\n",
    "        count = 1\n",
    "        for name in product_names:\n",
    "            if count <= 50:\n",
    "                name = name.text.lower()\n",
    "                predefined_name = \"la shield\"\n",
    "                \n",
    "                if count <= 10 or predefined_name in name:\n",
    "                    Keyword_id.append(keyword_id)\n",
    "                    \n",
    "                    search_category.append(0)\n",
    "                    \n",
    "                    Product_name.append(name)\n",
    "                    Ranking.append(count)\n",
    "\n",
    "                    today_date = date.today()\n",
    "                    Date.append(today_date)\n",
    "        #             print(today_date)\n",
    "\n",
    "                    Keyword.append(keyword)\n",
    "        #             print(keyword)\n",
    "            \n",
    "\n",
    "#                 elif predefined_name in name:\n",
    "#                     Keyword_id.append(keyword_id)\n",
    "                    \n",
    "#                     search_category.append(0)\n",
    "                    \n",
    "#                     Product_name.append(name)\n",
    "#                     Ranking.append(count)\n",
    "\n",
    "#                     today_date = date.today()\n",
    "#                     Date.append(today_date)\n",
    "#         #             print(today_date)\n",
    "\n",
    "#                     Keyword.append(keyword)\n",
    "#         #             print(keyword)\n",
    "        \n",
    "                else:\n",
    "                    pass\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # print(Platform)\n",
    "    # print(Date)\n",
    "    # print(Keyword)\n",
    "    # print(Product_name)\n",
    "    # print(Ranking)\n",
    "    # print(Url)\n",
    "\n",
    "    df = pd.DataFrame({\"keyword_id\": Keyword_id,\n",
    "                       \"search_category\": search_category,\n",
    "                       \"product_name\": Product_name,\n",
    "                       \"product_rank\": Ranking,\n",
    "                       \"crawl_date\": Date,\n",
    "                       \"platform_id\": 1,\n",
    "                       \"area_id\": pincode})\n",
    "    print(df)\n",
    "    \n",
    "    df.to_sql('product_keywordsearchresult', engine, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Method for fetching product details and storing it\n",
    "# Takes 3 inputs\n",
    "# 1. Url whose soup object needs to be created\n",
    "# 2. List of keywords which are predefined\n",
    "# 3. List of product names for comparison (predefined)\n",
    "# The method gathers the required data, creates a dataframe and stores the data in a csv.\n",
    "def flipkart_product_search(urls, flipkart_keywords_df, pincode):\n",
    "    Keyword_id = []\n",
    "    search_category = []\n",
    "    Date = []\n",
    "    Keyword = []\n",
    "    Product_name = []\n",
    "    Ranking = []\n",
    "    \n",
    "    keywords = flipkart_keywords_df['keyword'].tolist()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        single_keyword_df = flipkart_keywords_df[flipkart_keywords_df['keyword']==keyword]\n",
    "        keyword_id = int(single_keyword_df['id'])\n",
    "#         print(keyword_id)\n",
    "#         print(\"after keyword_id >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        \n",
    "        soup = get_soup(urls[keyword])\n",
    "\n",
    "        product_names = soup.find_all(fl_tag_name_for_product_names, class_=fl_class_for_product_names)\n",
    "#         print('printing product names>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "#         print(product_names)\n",
    "        count = 1\n",
    "        for name in product_names:\n",
    "            if count <= 50:\n",
    "                name = name.text.lower()\n",
    "                predefined_name = \"la shield\"\n",
    "                \n",
    "                if count <= 10 or predefined_name in name:\n",
    "                    Keyword_id.append(keyword_id)\n",
    "                    \n",
    "                    search_category.append(0)\n",
    "                    \n",
    "                    Product_name.append(name)\n",
    "                    Ranking.append(count)\n",
    "\n",
    "                    today_date = date.today()\n",
    "                    Date.append(today_date)\n",
    "        #             print(today_date)\n",
    "\n",
    "                    Keyword.append(keyword)\n",
    "        #             print(keyword)\n",
    "            \n",
    "\n",
    "#                 elif predefined_name in name:\n",
    "#                     Keyword_id.append(keyword_id)\n",
    "                    \n",
    "#                     search_category.append(0)\n",
    "                    \n",
    "#                     Product_name.append(name)\n",
    "#                     Ranking.append(count)\n",
    "\n",
    "#                     today_date = date.today()\n",
    "#                     Date.append(today_date)\n",
    "#         #             print(today_date)\n",
    "\n",
    "#                     Keyword.append(keyword)\n",
    "#         #             print(keyword)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # print(Platform)\n",
    "    # print(Date)\n",
    "    # print(Keyword)\n",
    "    # print(Product_name)\n",
    "    # print(Ranking)\n",
    "    # print(Url)\n",
    "\n",
    "    df = pd.DataFrame({\"keyword_id\": Keyword_id,\n",
    "                       \"search_category\": search_category,\n",
    "                       \"product_name\": Product_name,\n",
    "                       \"product_rank\": Ranking,\n",
    "                       \"crawl_date\": Date,\n",
    "                       \"platform_id\": 2,\n",
    "                       \"area_id\": pincode})\n",
    "    print(df)\n",
    "    df.to_sql('product_keywordsearchresult', engine, if_exists='append', index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "# Method for fetching product details and storing it\n",
    "# Takes 3 inputs\n",
    "# 1. Url whose soup object needs to be created\n",
    "# 2. List of keywords which are predefined\n",
    "# 3. List of product names for comparison (predefined)\n",
    "# The method gathers the required data, creates a dataframe and stores the data in a csv.\n",
    "def nykaa_product_search(urls, nykaa_keywords_df, pincode):\n",
    "    Keyword_id = []\n",
    "    search_category = []\n",
    "    Date = []\n",
    "    Keyword = []\n",
    "    Product_name = []\n",
    "    Ranking = []\n",
    "    \n",
    "    keywords = nykaa_keywords_df['keyword'].tolist()\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        \n",
    "        single_keyword_df = nykaa_keywords_df[nykaa_keywords_df['keyword']==keyword]\n",
    "        keyword_id = int(single_keyword_df['id'])\n",
    "#         print(keyword_id)\n",
    "#         print(\"after keyword_id >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "\n",
    "#         driver = webdriver.Chrome('chromedriver_win32')\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(urls[keyword])\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        driver.quit()\n",
    "\n",
    "        product_names = soup.find_all(ny_tag_name_for_product_names, class_=ny_class_for_product_names)\n",
    "        \n",
    "        count = 1\n",
    "        for name in product_names:\n",
    "            if count <= 50:\n",
    "                name = name.text.lower()\n",
    "                predefined_name = \"la shield\"\n",
    "                \n",
    "                if count <= 10 or predefined_name in name:\n",
    "                    Keyword_id.append(keyword_id)\n",
    "                    \n",
    "                    search_category.append(0)\n",
    "                    \n",
    "                    Product_name.append(name)\n",
    "                    Ranking.append(count)\n",
    "\n",
    "                    today_date = date.today()\n",
    "                    Date.append(today_date)\n",
    "        #             print(today_date)\n",
    "\n",
    "                    Keyword.append(keyword)\n",
    "        #             print(keyword)\n",
    "            \n",
    "\n",
    "#                 elif predefined_name in name:\n",
    "#                     Keyword_id.append(keyword_id)\n",
    "                    \n",
    "#                     search_category.append(0)\n",
    "                    \n",
    "#                     Product_name.append(name)\n",
    "#                     Ranking.append(count)\n",
    "\n",
    "#                     today_date = date.today()\n",
    "#                     Date.append(today_date)\n",
    "#         #             print(today_date)\n",
    "\n",
    "#                     Keyword.append(keyword)\n",
    "#         #             print(keyword)\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "                count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # print(Platform)\n",
    "    # print(Date)\n",
    "    # print(Keyword)\n",
    "    # print(Product_name)\n",
    "    # print(Ranking)\n",
    "    # print(Url)\n",
    "\n",
    "    df = pd.DataFrame({\"keyword_id\": Keyword_id,\n",
    "                       \"search_category\": search_category,\n",
    "                       \"product_name\": Product_name,\n",
    "                       \"product_rank\": Ranking,\n",
    "                       \"crawl_date\": Date,\n",
    "                       \"platform_id\": 3,\n",
    "                       \"area_id\": pincode})\n",
    "    \n",
    "    print(df)\n",
    "    df.to_sql('product_keywordsearchresult', engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def keyword_search():\n",
    "    try:\n",
    "        query = text('select * from \"product_keywordtblarea\" where \"is_active\"=:val1')\n",
    "        params = {'val1':1}\n",
    "        products_df = pd.read_sql_query(query, engine, params=params)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    amazon_products_df = products_df[products_df['platform_id']==1]\n",
    "    flipkart_products_df = products_df[products_df['platform_id']==2]\n",
    "    nykaa_products_df = products_df[products_df['platform_id']==3]\n",
    "#     print(amazon_products_df)\n",
    "#     print(flipkart_products_df)\n",
    "#     print(nykaa_products_df)\n",
    "#     print(\"after df >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "\n",
    "\n",
    "# Amazon code begins here //////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    amazon_keywords_df = pd.DataFrame()\n",
    "\n",
    "    for keyword_id in amazon_products_df['keyword_id']:\n",
    "        query = text('select * from \"product_keywordtbl\" where \"id\"=:val1')\n",
    "        params = {'val1':int(keyword_id)}\n",
    "        keywords_df = pd.read_sql_query(query, engine, params=params)\n",
    "#         print(keywords_df)\n",
    "#         print(\"after keywords_df>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        amazon_keywords_df = pd.concat([amazon_keywords_df, keywords_df])\n",
    "    \n",
    "    amazon_keywords = amazon_keywords_df['keyword'].tolist()\n",
    "#     print(amazon_keywords)\n",
    "#     print(\"after keywords >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        \n",
    "    try:\n",
    "        for pincode in amazon_products_df['area_id']:\n",
    "            keyword_urls_amz = get_amazon_links(amazon_keywords, pincode)\n",
    "            amazon_product_search(keyword_urls_amz, amazon_keywords_df, pincode)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "# Amazon code ends here ////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Flipkart code begins here //////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    flipkart_keywords_df = pd.DataFrame()\n",
    "\n",
    "    for keyword_id in flipkart_products_df['keyword_id']:\n",
    "        query = text('select * from \"product_keywordtbl\" where \"id\"=:val1')\n",
    "        params = {'val1':int(keyword_id)}\n",
    "        keywords_df = pd.read_sql_query(query, engine, params=params)\n",
    "#         print(keywords_df)\n",
    "#         print(\"after keywords_df>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        flipkart_keywords_df = pd.concat([flipkart_keywords_df, keywords_df])\n",
    "    \n",
    "    flipkart_keywords = flipkart_keywords_df['keyword'].tolist()\n",
    "#     print(flipkart_keywords)\n",
    "#     print(\"after keywords >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        \n",
    "    try:\n",
    "        for pincode in flipkart_products_df['area_id']:\n",
    "            keyword_urls_fl = get_flipkart_links(flipkart_keywords, pincode)\n",
    "            flipkart_product_search(keyword_urls_fl, flipkart_keywords_df, pincode)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "# Flipkart code ends here ////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Nykaa code begins here //////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "    nykaa_keywords_df = pd.DataFrame()\n",
    "\n",
    "    for keyword_id in nykaa_products_df['keyword_id']:\n",
    "        query = text('select * from \"product_keywordtbl\" where \"id\"=:val1')\n",
    "        params = {'val1':int(keyword_id)}\n",
    "        keywords_df = pd.read_sql_query(query, engine, params=params)\n",
    "#         print(keywords_df)\n",
    "#         print(\"after keywords_df>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        nykaa_keywords_df = pd.concat([nykaa_keywords_df, keywords_df])\n",
    "    \n",
    "    nykaa_keywords = nykaa_keywords_df['keyword'].tolist()\n",
    "#     print(nykaa_keywords)\n",
    "#     print(\"after keywords >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        \n",
    "    try:\n",
    "        for pincode in nykaa_products_df['area_id']:\n",
    "            keyword_urls_ny = get_nykaa_links(nykaa_keywords, pincode)\n",
    "            nykaa_product_search(keyword_urls_ny, nykaa_keywords_df, pincode)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "# Nykaa code ends here ////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "        \n",
    "keyword_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c72fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc43c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
